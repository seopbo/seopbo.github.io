---
tags:
  - NLP
  - ContrastiveLearning
  - InBatchNegatives
  - LargeBatch
---

import "katex/dist/katex.min.css";

# GradCache
[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)에서 소개된 방법으로, batch-wise로 loss를 계산하는 in batch negatives 방식의 contrastive learning 시, gradient accumulation을 수행할 수 있게 해주는 방법이다. (일반적인 gradient accumulation은 loss 자체가 batch-wise로 계산되지 않는 경우에 수행이 가능함. ) 이 기법을 이용하면 in batch negatives 방식의 contrastive learning 시 large batch를 사용하는 것이 가능해진다.

## Methods
### Motivation
in batch negatives 방식의 contrastive learning의 backpropagation을 `from loss to representation`과 `from representation to model parameter`, 두 개로 나누는 것이 핵심이다. 이때 `from representation to model parameter` 부분은 전자가 주어질 경우 batch examples와 독립적으로 계산할 수 있다. loss 부분은 아래와 같은 수식으로 표현할 수 있다.

$$
L=-\frac{1}{|S|}\sum_{s_i \in S}\log\frac{\exp(f(s_i)^Tg(t_{r_i}))}{\sum_{t_j \in T}\exp(f(s_i)^Tg(t_j))}
$$

- $f$, $g$는 서로 다른 class의 data (e.g. question, paragraph)를 encoding하는 encoder
  - $f$의 parameter $\Theta$, $g$의 parameter $\Lambda$
- $\mathbf{S}$, $\mathbf{T}$는 각각의 class에 해당하는 dataset 일 때,
  - $S \subset \mathbf{S}$, $T \subset \mathbf{T}$
  -  $s \in S$, $t \in T$
  -  $s_i \in S$와 관련있는 데이터 $t_{r_i} \in T$

위의 수식으로부터 각각의 encoder $f$, $g$의 parameter를 update하기위한 gradient는 아래의 수식과 같이 계산됨을 확인할 수 있다.

$$
\frac{\partial L}{\partial \Theta}=\sum_{s_i \in S} \frac{\partial L}{\partial f(s_i)} \frac{\partial f(s_i)}{\partial \Theta}
$$
$$
\frac{\partial L}{\partial \Lambda}=\sum_{t_j \in T} \frac{\partial L}{\partial g(t_j)} \frac{\partial g(t_j)}{\partial \Lambda}
$$
$$
\frac{\partial L}{\partial f(s_i)}=-\frac{1}{|S|}\bigg( g(t_{r_i})-\sum_{t_j \in T}p_{ij}g(t_j)\bigg)
$$
$$
\frac{\partial L}{\partial g(t_j)}=-\frac{1}{|S|}\bigg(\epsilon_j - \sum_{s_i \in S}p_{ij}f(s_i)\bigg) \quad \text{where} \quad \epsilon_j=\begin{cases}
   f(s_k) &\text{if} \space \exists \space k \space \text{s.t} \space r_k=j \\
   0 &\text{otherwise}
\end{cases}
$$
$$
p_{ij}=\frac{\exp(f(s_i)^Tg(t_j))}{\sum_{t \in T}\exp(f(s_i)^Tg(t))}
$$
$$
$$
위의 식을 아래의 두 가지를 확인할 수 있다.

- `from representation to model parameter`에 해당하는 $\frac{\partial f(s_i)}{\partial \Theta}$, $\frac{\partial g(t_j)}{\partial \Lambda}$는 $s_i$, $\Theta$, $t_j$, $\Lambda$에만 의존한다.
- encoder의 `from loss to representation` 부분인 $\frac{\partial L}{\partial f(s_i)}$, $\frac{\partial L}{\partial g(t_j)}$는 각각의 representation에만 의존한다.


이 사실을 토대로 아래처럼 GradCache를 구현한다.

### Implementation
batch $\mathbb{S}$, $\mathbb{T}$가 주어져있을 때, 이를 아래와 같이 sub-batch로 생각한다.

- $\mathbb{S}=\{\hat{S}_1,\hat{S}_2,...\}$, $\mathbb{T}=\{\hat{T}_1,\hat{T}_2,...\}$

#### Step 1: Graph-less Forward
위의  각각의 sub-batch에 대해서 computation graph를 만들지 않고, forward pass를 수행한다. 이 때 계산된 모든 representation들을 저장해둔다.

#### Step 2: Representation Gradient Computation
step 1에서 계산된 representation들을 토대로 computation graph를 만들어서 contrastive loss를 계산한다. 이를 통해 `from loss to representation`에 부분에 대응하는 `Representation Gradient Cache`를 계산한다.

$$
\text{Representation Gradient Cache}=[\mathbf{u}_1,\mathbf{u}_2,...,\mathbf{v}_1,\mathbf{v}_2,...]
$$
$$
\mathbf{u}_i=\frac{\partial L}{\partial f(s_i)}, \space \mathbf{v}_i=\frac{\partial L}{\partial g(t_i)}
$$

#### Step 3: Sub-batch Gradient Accumulation
각각의 sub-batch에 대해서 computation graph를 만들면서 다시 forward를 수행한다. 이를 토대로 `from representation to model parameter` 부분에 대응하는 gradient인 $\frac{\partial f(s_i)}{\partial \Theta}$, $\frac{\partial g(t_j)}{\partial \Lambda}$를 계산할 수 있다. 이때 upstream gradient인 $\frac{\partial L}{\partial f(s_i)}$, $\frac{\partial L}{\partial g(t_i)}$와 local gradient인 $\frac{\partial f(s_i)}{\partial \Theta}$, $\frac{\partial g(t_j)}{\partial \Lambda}$를 곱하여 `from loss to model parameter` 부분에 대응하는 parameter update에 필요한 gradient를 구할 수 있고, step 1과 step 2에서 이를 sub-batch단위로 계산했으므로 최종적으로 아래의 수식처럼 정리된다.

$$
\frac{\partial L}{\partial \Theta}=\sum_{\hat{S}_j \subset \mathbb{S}}\sum_{s_i \in \hat{S}_j}\frac{\partial L}{\partial f(s_i)}\frac{\partial f(s_i)}{\partial \Theta}=\sum_{\hat{S}_j \subset \mathbb{S}}\sum_{s_i \in \hat{S}_j}\mathbf{u}_i\frac{\partial f(s_i)}{\partial \Theta}
$$
$$
\frac{\partial L}{\partial \Lambda}=\sum_{\hat{T}_j \subset \mathbb{T}}\sum_{t_i \in \hat{T}_j}\frac{\partial L}{\partial g(t_i)}\frac{\partial g(t_i)}{\partial \Lambda}=\sum_{\hat{T}_j \subset \mathbb{T}}\sum_{t_i \in \hat{T}_j}\mathbf{v}_i\frac{\partial g(t_i)}{\partial \Lambda}
$$

#### Step 4: Optimization
Step 3에서 계산한 `from loss to model` 부분에 대응하는 gradient로 parameter update를 수행한다. 이 방식으로는 아래와 같은 이점이 있다.

> Compared to directly updating with the full batch, which requires memory linear to the number of examples, our method fixes the number of examples in each encoder gradient computation to be the size of sub-batch and therefore requires constant memory for encoder forward-backward pass.

## Thought
sub-batch를 다시 forward해야하는 점에서는 activation checkpointing 방법과 유사한 점이 존재하는 것 같다. activation checkpointing과 기존의 non batch-wise loss를 사용하는 학습에서 활용되는 gradient accumulation이 동시에 조합된 방식인 것 같다.

## References
- https://arxiv.org/abs/2101.06983
- https://github.com/luyug/GradCache
  - 논문의 저자가 package 형태로 손쉽게 사용할 수 있도록 구현해두었다.

## Links
TBD
