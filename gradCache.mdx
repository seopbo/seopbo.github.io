---
tags:
  - NLP
  - ContrastiveLearning
  - InBatchNegatives
  - LargeBatch
---

# GradCache
[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)에서 소개된 방법으로, batch-wise로 loss를 계산하는 in batch negatives 방식의 contrastive learning 시, gradient accumulation을 수행할 수 있게 해주는 방법이다. (일반적인 gradient accumulation은 loss 자체가 batch-wise로 계산되지 않는 경우에 수행이 가능함. ) 이 기법을 이용하면 in batch negatives 방식의 contrastive learning 시 large batch를 사용하는 것이 가능해진다.

## Methods
in batch negatives 방식의 contrastive learning의 backpropagation을 `from loss to representation`과 `from representation to model parameter`, 두 개로 나누는 것이 핵심이다. 이때 `from representation to model parameter` 부분은 전자가 주어질 경우 batch examples와 독립적으로 계산할 수 있다.

$$
L=-\frac{1}{|S|}\sum_{s_i \in S}\log\frac{\exp(f(s_i)^Tg(t_{r_i}))}{\sum_{t_j \in T}\exp(f(s_i)^Tg(t_j))} \\
\mathbf{T}
$$

### Step 1: Graph-less Forward

### Step 2: 

## References
- https://arxiv.org/abs/2101.06983


## Links
TBD
