{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/","title":"Everything I log"},"frontmatter":{"draft":false},"rawBody":"---\nseoTitle: Everything I log by seopbo - A Personal Knowledge Based Wiki\ntags:\n  - Me\n---\n# Everything I log\n- [Read the Latest Updates](https://seopbo.github.io/latest/)\n\n## Why this build?\n습득한 지식을 체계적으로 정리하기위해서 블로그보다 위키스타일이 적합하다고 생각하여 만들게되었습니다. 글 작성 시 [obsidian](https://obsidian.md/)을 사용합니다.\n\n## How I build this?\n[gatsby-theme-primer-wiki](https://github.com/theowenyoung/gatsby-theme-primer-wiki)로 build하여 static web page로 deploy할 수 있습니다. reference로는 해당 theme의 author의 [wiki](https://wiki.owenyoung.com/)를 참고하기 좋습니다.\n\n## About Me\n`python`을 주로 사용하는 research engineer입니다. 주로 `nlp`와 관련된 연구 및 개발을 수행하고 있습니다. 항상 좋은 개발자가 되기위해 노력하고 있습니다.\n\n- [[about]]\n\n## Contact me\n- Email: bsk0130@gmail.com\n\n[about]: about.md \"About Me and Career\""},{"fields":{"slug":"/about/","title":"About Me and Career"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - Me\n---\n# About Me and Career\n## About Me\n고양이를 좋아하는 보통의 개발자\n\n## About Career\n[detail](https://drive.google.com/file/d/1poW_GQ6OikBP2ZVD9jmfLh6UlmkBFi49/view?usp=sharing)은 클릭, 아래는 간략하게\n\n### Technical Interest\n- Natural Language Processing, Natural Language Understanding, Information Retrieval\n- Pre-trained Language Models (PLM), Large Language Models (LLM)\n\n### Industry Experience\n- NAVER\n    - Research engineer (2020.03 ~ 2022.04)\n-  LG Electronics\n    -  Research engineer (2018.08 ~ 2020.02)\n-  KT\n    -  Research engineer (2017.06 ~ 2018.07)\n\n### Publications\n- [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://aclanthology.org/2021.emnlp-main.274) (Kim et al., EMNLP 2021)\n- [Integrating cluster validity indices based on data envelopment analysis](https://www.sciencedirect.com/science/article/abs/pii/S1568494617307202) (Kim et al., Applied Soft Computing 64 (2018))\n"},{"fields":{"slug":"/gradCache/","title":"GradCache"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - NLP\n  - ContrastiveLearning\n  - InBatchNegatives\n  - LargeBatch\n---\n\nimport \"katex/dist/katex.min.css\";\n\n# GradCache\n[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)에서 소개된 방법으로, batch-wise로 loss를 계산하는 in batch negatives 방식의 contrastive learning 시, gradient accumulation을 수행할 수 있게 해주는 방법이다. (일반적인 gradient accumulation은 loss 자체가 batch-wise로 계산되지 않는 경우에 수행이 가능함. ) 이 기법을 이용하면 in batch negatives 방식의 contrastive learning 시 large batch를 사용하는 것이 가능해진다.\n\n## Methods\nin batch negatives 방식의 contrastive learning의 backpropagation을 `from loss to representation`과 `from representation to model parameter`, 두 개로 나누는 것이 핵심이다. 이때 `from representation to model parameter` 부분은 전자가 주어질 경우 batch examples와 독립적으로 계산할 수 있다.\n\n$$\nL=-\\frac{1}{|S|}\\sum_{s_i \\in S}\\log\\frac{\\exp(f(s_i)^Tg(t_{r_i}))}{\\sum_{t_j \\in T}\\exp(f(s_i)^Tg(t_j))}\n$$\n\n### Step 1: Graph-less Forward\n\n### Step 2: \n\n## References\n- https://arxiv.org/abs/2101.06983\n\n## Links\nTBD\n"},{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"}]}}}