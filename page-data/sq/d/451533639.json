{"data":{"allMdx":{"nodes":[{"fields":{"slug":"/","title":"Everything I log"},"frontmatter":{"draft":false},"rawBody":"---\nseoTitle: Everything I log by seopbo - A Personal Knowledge Based Wiki\ntags:\n  - Me\n---\n# Everything I log\n- [Read the Latest Updates](https://seopbo.github.io/latest/)\n\n## Why this build?\n습득한 지식을 체계적으로 정리하기위해서 블로그보다 위키스타일이 적합하다고 생각하여 만들게되었습니다. 글 작성 시 [obsidian](https://obsidian.md/)을 사용합니다.\n\n## How I build this?\n[gatsby-theme-primer-wiki](https://github.com/theowenyoung/gatsby-theme-primer-wiki)로 build하여 static web page로 deploy할 수 있습니다. reference로는 해당 theme의 author의 [wiki](https://wiki.owenyoung.com/)를 참고하기 좋습니다.\n\n## About Me\n`python`을 주로 사용하는 research engineer입니다. 주로 `nlp`와 관련된 연구 및 개발을 수행하고 있습니다. 항상 좋은 개발자가 되기위해 노력하고 있습니다.\n\n- [[About]]\n\n## Contact me\n- Email: bsk0130@gmail.com\n\n[About]: About.mdx \"About Me and Career\""},{"fields":{"slug":"/about/","title":"About Me and Career"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - Me\n---\n# About Me and Career\n## About Me\n고양이를 좋아하는 보통의 개발자\n\n## About Career\n[detail](https://drive.google.com/file/d/1poW_GQ6OikBP2ZVD9jmfLh6UlmkBFi49/view?usp=sharing)은 클릭, 아래는 간략하게\n\n### Technical Interest\n- Natural Language Processing, Natural Language Understanding, Information Retrieval\n- Pre-trained Language Models (PLM), Large Language Models (LLM)\n\n### Industry Experience\n- NAVER\n    - Research engineer (2020.03 ~ 2022.04)\n-  LG Electronics\n    -  Research engineer (2018.08 ~ 2020.02)\n-  KT\n    -  Research engineer (2017.06 ~ 2018.07)\n\n### Publications\n- [What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers](https://aclanthology.org/2021.emnlp-main.274) (Kim et al., EMNLP 2021)\n- [Integrating cluster validity indices based on data envelopment analysis](https://www.sciencedirect.com/science/article/abs/pii/S1568494617307202) (Kim et al., Applied Soft Computing 64 (2018))\n"},{"fields":{"slug":"/gradCache/","title":"GradCache"},"frontmatter":{"draft":false},"rawBody":"---\ntags:\n  - NLP\n  - ContrastiveLearning\n  - InBatchNegatives\n  - LargeBatch\n---\n\nimport \"katex/dist/katex.min.css\";\n\n# GradCache\n[Scaling Deep Contrastive Learning Batch Size under Memory Limited Setup](https://arxiv.org/abs/2101.06983)에서 소개된 방법으로, batch-wise로 loss를 계산하는 in batch negatives 방식의 contrastive learning 시, gradient accumulation을 수행할 수 있게 해주는 방법이다. (일반적인 gradient accumulation은 loss 자체가 batch-wise로 계산되지 않는 경우에 수행이 가능함. ) 이 기법을 이용하면 in batch negatives 방식의 contrastive learning 시 large batch를 사용하는 것이 가능해진다.\n\n## Methods\n### Motivation\nin batch negatives 방식의 contrastive learning의 backpropagation을 `from loss to representation`과 `from representation to model parameter`, 두 개로 나누는 것이 핵심이다. 이때 `from representation to model parameter` 부분은 전자가 주어질 경우 batch examples와 독립적으로 계산할 수 있다. loss 부분은 아래와 같은 수식으로 표현할 수 있다.\n\n$$\nL=-\\frac{1}{|S|}\\sum_{s_i \\in S}\\log\\frac{\\exp(f(s_i)^Tg(t_{r_i}))}{\\sum_{t_j \\in T}\\exp(f(s_i)^Tg(t_j))}\n$$\n\n- $f$, $g$는 서로 다른 class의 data (e.g. question, paragraph)를 encoding하는 encoder\n  - $f$의 parameter $\\Theta$, $g$의 parameter $\\Lambda$\n- $\\mathbf{S}$, $\\mathbf{T}$는 각각의 class에 해당하는 dataset 일 때,\n  - $S \\subset \\mathbf{S}$, $T \\subset \\mathbf{T}$\n  -  $s \\in S$, $t \\in T$\n  -  $s_i \\in S$와 관련있는 데이터 $t_{r_i} \\in T$\n\n위의 수식으로부터 각각의 encoder $f$, $g$의 parameter를 update하기위한 gradient는 아래의 수식과 같이 계산됨을 확인할 수 있다.\n\n$$\n\\frac{\\partial L}{\\partial \\Theta}=\\sum_{s_i \\in S} \\frac{\\partial L}{\\partial f(s_i)} \\frac{\\partial f(s_i)}{\\partial \\Theta}\n$$\n$$\n\\frac{\\partial L}{\\partial \\Lambda}=\\sum_{t_j \\in T} \\frac{\\partial L}{\\partial g(t_j)} \\frac{\\partial g(t_j)}{\\partial \\Lambda}\n$$\n$$\n\\frac{\\partial L}{\\partial f(s_i)}=-\\frac{1}{|S|}\\bigg( g(t_{r_i})-\\sum_{t_j \\in T}p_{ij}g(t_j)\\bigg)\n$$\n$$\n\\frac{\\partial L}{\\partial g(t_j)}=-\\frac{1}{|S|}\\bigg(\\epsilon_j - \\sum_{s_i \\in S}p_{ij}f(s_i)\\bigg) \\quad \\text{where} \\quad \\epsilon_j=\\begin{cases}\n   f(s_k) &\\text{if} \\space \\exists \\space k \\space \\text{s.t} \\space r_k=j \\\\\n   0 &\\text{otherwise}\n\\end{cases}\n$$\n$$\np_{ij}=\\frac{\\exp(f(s_i)^Tg(t_j))}{\\sum_{t \\in T}\\exp(f(s_i)^Tg(t))}\n$$\n$$\n$$\n위의 식을 아래의 두 가지를 확인할 수 있다.\n\n- `from representation to model parameter`에 해당하는 $\\frac{\\partial f(s_i)}{\\partial \\Theta}$, $\\frac{\\partial g(t_j)}{\\partial \\Lambda}$는 $s_i$, $\\Theta$, $t_j$, $\\Lambda$에만 의존한다.\n- encoder의 `from loss to representation` 부분인 $\\frac{\\partial L}{\\partial f(s_i)}$, $\\frac{\\partial L}{\\partial g(t_j)}$는 각각의 representation에만 의존한다.\n\n\n이 사실을 토대로 아래처럼 GradCache를 구현한다.\n\n### Implementation\nbatch $\\mathbb{S}$, $\\mathbb{T}$가 주어져있을 때, 이를 아래와 같이 sub-batch로 생각한다.\n\n- $\\mathbb{S}=\\{\\hat{S}_1,\\hat{S}_2,...\\}$, $\\mathbb{T}=\\{\\hat{T}_1,\\hat{T}_2,...\\}$\n\n#### Step 1: Graph-less Forward\n위의  각각의 sub-batch에 대해서 computation graph를 만들지 않고, forward pass를 수행한다. 이 때 계산된 모든 representation들을 저장해둔다.\n\n#### Step 2: Representation Gradient Computation\nstep 1에서 계산된 representation들을 토대로 computation graph를 만들어서 contrastive loss를 계산한다. 이를 통해 `from loss to representation`에 부분에 대응하는 `Representation Gradient Cache`를 계산한다.\n\n$$\n\\text{Representation Gradient Cache}=[\\mathbf{u}_1,\\mathbf{u}_2,...,\\mathbf{v}_1,\\mathbf{v}_2,...]\n$$\n$$\n\\mathbf{u}_i=\\frac{\\partial L}{\\partial f(s_i)}, \\space \\mathbf{v}_i=\\frac{\\partial L}{\\partial g(t_i)}\n$$\n\n#### Step 3: Sub-batch Gradient Accumulation\n각각의 sub-batch에 대해서 computation graph를 만들면서 다시 forward를 수행한다. 이를 토대로 `from representation to model parameter` 부분에 대응하는 gradient인 $\\frac{\\partial f(s_i)}{\\partial \\Theta}$, $\\frac{\\partial g(t_j)}{\\partial \\Lambda}$를 계산할 수 있다. 이때 upstream gradient인 $\\frac{\\partial L}{\\partial f(s_i)}$, $\\frac{\\partial L}{\\partial g(t_i)}$와 local gradient인 $\\frac{\\partial f(s_i)}{\\partial \\Theta}$, $\\frac{\\partial g(t_j)}{\\partial \\Lambda}$를 곱하여 `from loss to model parameter` 부분에 대응하는 parameter update에 필요한 gradient를 구할 수 있고, step 1과 step 2에서 이를 sub-batch단위로 계산했으므로 최종적으로 아래의 수식처럼 정리된다.\n\n$$\n\\frac{\\partial L}{\\partial \\Theta}=\\sum_{\\hat{S}_j \\subset \\mathbb{S}}\\sum_{s_i \\in \\hat{S}_j}\\frac{\\partial L}{\\partial f(s_i)}\\frac{\\partial f(s_i)}{\\partial \\Theta}=\\sum_{\\hat{S}_j \\subset \\mathbb{S}}\\sum_{s_i \\in \\hat{S}_j}\\mathbf{u}_i\\frac{\\partial f(s_i)}{\\partial \\Theta}\n$$\n$$\n\\frac{\\partial L}{\\partial \\Lambda}=\\sum_{\\hat{T}_j \\subset \\mathbb{T}}\\sum_{t_i \\in \\hat{T}_j}\\frac{\\partial L}{\\partial g(t_i)}\\frac{\\partial g(t_i)}{\\partial \\Lambda}=\\sum_{\\hat{T}_j \\subset \\mathbb{T}}\\sum_{t_i \\in \\hat{T}_j}\\mathbf{v}_i\\frac{\\partial g(t_i)}{\\partial \\Lambda}\n$$\n\n#### Step 4: Optimization\nStep 3에서 계산한 `from loss to model` 부분에 대응하는 gradient로 parameter update를 수행한다. 이 방식으로는 아래와 같은 이점이 있다.\n\n> Compared to directly updating with the full batch, which requires memory linear to the number of examples, our method fixes the number of examples in each encoder gradient computation to be the size of sub-batch and therefore requires constant memory for encoder forward-backward pass.\n\n## Thought\nsub-batch를 다시 forward해야하는 점에서는 activation checkpointing 방법과 유사한 점이 존재하는 것 같다. activation checkpointing과 기존의 non batch-wise loss를 사용하는 학습에서 활용되는 gradient accumulation이 동시에 조합된 방식인 것 같다.\n\n## References\n- https://arxiv.org/abs/2101.06983\n- https://github.com/luyug/GradCache\n  - 논문의 저자가 package 형태로 손쉽게 사용할 수 있도록 구현해두었다.\n\n## Links\nTBD\n"},{"fields":{"slug":"/placeholder/","title":"This Is a Placeholder File for Mdx"},"frontmatter":{"draft":true},"rawBody":"---\ntitle: This Is a Placeholder File for Mdx\ndraft: true\ntags:\n  - gatsby-theme-primer-wiki-placeholder\n---\n"}]}}}